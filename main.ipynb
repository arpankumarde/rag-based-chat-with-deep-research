{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c73e64-6e71-4fec-8b2b-231181d859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community langchain-google-genai chromadb docling python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d94268-3459-4612-b316-e7d0dbe431e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "# Core imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LangGraph imports for modern memory management\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Document processing\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Set up environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "\n",
    "class RAGChatSystem:\n",
    "    def __init__(self, temperature=0.5, top_p=0.9):\n",
    "        \"\"\"Initialize the RAG chat system with LangGraph persistence.\"\"\"\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            temperature=0.3,\n",
    "            top_p=top_p,\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200, length_function=len\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "        self.document_converter = DocumentConverter()\n",
    "\n",
    "        self.memory = MemorySaver()\n",
    "        self.app = None\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def process_documents(self, file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"Process documents using Docling and return LangChain documents.\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                print(f\"Processing: {file_path}\")\n",
    "\n",
    "                # Convert document using Docling\n",
    "                result = self.document_converter.convert(file_path)\n",
    "\n",
    "                # Extract text content\n",
    "                text_content = result.document.export_to_markdown()\n",
    "\n",
    "                # Create LangChain document\n",
    "                doc = Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata={\"source\": file_path, \"filename\": Path(file_path).name},\n",
    "                )\n",
    "                documents.append(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"Create ChromaDB vector store from documents.\"\"\"\n",
    "        if not documents:\n",
    "            print(\"No documents to process!\")\n",
    "            return\n",
    "\n",
    "        # Split documents into chunks\n",
    "        texts = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(texts)} text chunks\")\n",
    "\n",
    "        # Create vector store\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=texts, embedding=self.embeddings, persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "\n",
    "        # Create LangGraph application\n",
    "        self._create_langgraph_app()\n",
    "\n",
    "        print(\"Vector store and LangGraph app created successfully!\")\n",
    "\n",
    "    def _create_langgraph_app(self):\n",
    "        \"\"\"Create LangGraph application with RAG capabilities.\"\"\"\n",
    "\n",
    "        def rag_agent(state: MessagesState):\n",
    "            \"\"\"RAG agent that retrieves context and generates responses.\"\"\"\n",
    "            # Get the last human message\n",
    "            last_message = state[\"messages\"][-1]\n",
    "            query = last_message.content\n",
    "\n",
    "            # Retrieve relevant documents\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "            relevant_docs = retriever.invoke(query)\n",
    "\n",
    "            # Create context from retrieved documents\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "            # Create RAG prompt\n",
    "            rag_prompt = f\"\"\"Based on the following context, answer the user's question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context provided. If the context doesn't contain relevant information, say so.\"\"\"\n",
    "\n",
    "            # Generate response using LLM\n",
    "            response = self.llm.invoke(rag_prompt)\n",
    "\n",
    "            # Return AIMessage with sources\n",
    "            sources_info = f\"\\n\\nSources: {len(relevant_docs)} documents used\"\n",
    "            return {\"messages\": [AIMessage(content=response + sources_info)]}\n",
    "\n",
    "        # Create the graph\n",
    "        graph_builder = StateGraph(MessagesState)\n",
    "        graph_builder.add_node(\"rag_agent\", rag_agent)\n",
    "        graph_builder.add_edge(START, \"rag_agent\")\n",
    "\n",
    "        # Compile with checkpointer for memory\n",
    "        self.app = graph_builder.compile(checkpointer=self.memory)\n",
    "\n",
    "    def chat(self, question: str) -> dict:\n",
    "        \"\"\"Chat with the RAG system using LangGraph.\"\"\"\n",
    "        if not self.app:\n",
    "            return {\"error\": \"Please upload and process documents first!\"}\n",
    "\n",
    "        try:\n",
    "            # Create config with thread ID for conversation persistence\n",
    "            config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "\n",
    "            # Invoke the graph with the user's question\n",
    "            result = self.app.invoke(\n",
    "                {\"messages\": [HumanMessage(content=question)]}, config=config\n",
    "            )\n",
    "\n",
    "            # Extract the response\n",
    "            ai_message = result[\"messages\"][-1]\n",
    "\n",
    "            return {\"answer\": ai_message.content, \"thread_id\": self.thread_id}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error generating response: {str(e)}\"}\n",
    "\n",
    "    def reset_chat_history(self):\n",
    "        \"\"\"Reset conversation by creating a new thread ID.\"\"\"\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "        print(f\"Chat history cleared! New thread ID: {self.thread_id}\")\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        \"\"\"Get current conversation history.\"\"\"\n",
    "        if not self.app:\n",
    "            return []\n",
    "\n",
    "        config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "        state = self.app.get_state(config)\n",
    "        return state.values.get(\"messages\", [])\n",
    "\n",
    "\n",
    "rag_system = RAGChatSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd536c6-801c-4cfd-856a-c7c02859a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(\n",
    "    self, folder_path: str, supported_extensions: List[str] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"Process all documents in a specified folder.\"\"\"\n",
    "    if supported_extensions is None:\n",
    "        supported_extensions = [\n",
    "            \".pdf\",\n",
    "            \".pptx\",\n",
    "            \".txt\",\n",
    "            \".docx\",\n",
    "            \".doc\",\n",
    "            \".md\",\n",
    "            \".html\",\n",
    "            \".csv\",\n",
    "            \".xlsx\",\n",
    "        ]\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: Folder '{folder_path}' does not exist!\")\n",
    "        return documents\n",
    "\n",
    "    # Get all files in the folder\n",
    "    file_paths = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if it's a file and has supported extension\n",
    "        if os.path.isfile(file_path):\n",
    "            file_extension = os.path.splitext(filename)[1].lower()\n",
    "            if file_extension in supported_extensions:\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "    print(f\"Found {len(file_paths)} supported files in '{folder_path}':\")\n",
    "    for fp in file_paths:\n",
    "        print(f\"  - {os.path.basename(fp)}\")\n",
    "\n",
    "    # Process all found files\n",
    "    if file_paths:\n",
    "        documents = self.process_documents(file_paths)\n",
    "    else:\n",
    "        print(\"No supported files found in the folder!\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447610d8-9901-4e3e-bf9c-e62b87547a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"Simple interactive chat interface.\"\"\"\n",
    "    print(\"RAG Chat System Ready! Type 'quit' to exit, 'reset' to clear history.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nYou: \").strip()\n",
    "\n",
    "        if question.lower() == \"quit\":\n",
    "            break\n",
    "        elif question.lower() == \"reset\":\n",
    "            rag_system.reset_chat_history()\n",
    "            continue\n",
    "        elif not question:\n",
    "            continue\n",
    "\n",
    "        response = rag_system.chat(question)\n",
    "\n",
    "        if \"error\" in response:\n",
    "            print(f\"Error: {response['error']}\")\n",
    "        else:\n",
    "            print(f\"\\nBot: {response['answer']}\")\n",
    "            print(f\"\\nSources used: {len(response['source_documents'])} documents\")\n",
    "\n",
    "\n",
    "# Start interactive chat\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97796a1f-6995-485b-b149-9b0446c7c644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
